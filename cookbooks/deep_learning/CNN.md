## CNN
[toc]
### 动机 
#### 稀疏交互
**稀疏交互**又叫**稀疏连接**或者**稀疏权重**。<br>
稀疏交互式相对于传统`DNN`来说的，在传统`DNN`中每一个输出单元于每一个输入单元产生交互，但是在`CNN`中，人工设定核大小远小于输入，即可产生稀疏交互。
<center>
<img src='http://p9l49hjew.bkt.clouddn.com/b2a57e4a3e75e6cacae424ba0a641cdc.jpg', width='300'/>
</center>


在上图中，图的上半部分表示一种稀疏交互，设定核的宽度为3，只有`$s_2, s_3, s_4$`受到`$x_3$`神经元影响；图的下半部分连接不在是稀疏的，所有的输出都会受到`$x_3$`的影响。<br>
这种方式思想是只占用几十到上百个像素点来检测一些小的有意义的特征，例如图像边缘。这个是基于Hubel and Wiesel在1962实验结果。<br>
<center>
<img src='http://p9l49hjew.bkt.clouddn.com/6fef6e8c66fc6714ebd26df123a4ea04.jpg', width='300'/>
</center>
从上图可以看出，在深度卷积神经网络中，在深层单元可能与绝大部分输入是间接交互的。

设定宽度远小于输入的核，产生稀疏交互有如下优点：
- 存储参数更少，提高系统计算效率。
> m: 输入神经元个数<br>
> n: 输出神经元个数<br>
> k: 卷积核宽度<br>
> 全连接参数个数为m * n<br>
> 稀疏连接参数个数为 k * n
- 深度神经元和绝大部分输入是间接交互

#### 参数共享
**参数共享**是指在一个模型的多个函数中使用相同的函数。<br>
在传统的`DNN`中，权重矩阵中的元素只使用一次，当乘以对应的输入后就不会再用到；在`CNN`中，用做一个输入的权重也会被绑定在其他权重上。
<center>
<img src='http://p9l49hjew.bkt.clouddn.com/51222851879f59265d020a3dc5557181.jpg', width='300'/>
</center>

上图表示了参数共享的过程。黑色箭头表示`$x_3$`神经元的对应的使用，在图的上半部分，也就是`CNN`中，由于参数共享，这个参数被用于所有的输入位置；在图的下半部分，这个参数仅仅对`$x_3$`使用。<br>
参数共享使得我们只需要学习一个参数集合，而不是对于每一个位置都要学习一个单独的参数集合，没有改变运行时间`(O(k*n))`但是显著地把模型参数需求降低至`k`个参数，并且`k`一般比`m`小很多数量级。<br>
参数共享使得神经网络具有**等变**的性质。若一个函数的满足输入改变，输出也以同样的方式改变这一性质，我们称之为**等变**。数学上表示如下:
```math
    f(g(x)) = g(f(x))
```
满足上式我们就说`$f(x)$`对变换`$g$`具有等变性，对于卷积来说，若令`$g$`为任意平移函数，那么卷积函数对`$g$`具有等变性。但是卷积函数对**放缩或者旋转不是等变**。

#### 等变表示
#### ==卷积提供了一种处理大小可变的输入的方法???==
